{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "content_based_using_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa217U9r0Ozp"
      },
      "source": [
        "## Content-Based Filtering Using Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCrYCZH0Ozq"
      },
      "source": [
        "This notebook relies on files created in the [content_based_preproc.ipynb](./content_based_preproc.ipynb) notebook. Be sure to run the code in there before completing this notebook.  \n",
        "Also, we'll be using the **python3** kernel from here on out so don't forget to change the kernel if it's still Python2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS-axvEx0Ozq"
      },
      "source": [
        "This lab illustrates:\n",
        "1. how to build feature columns for a model using tf.feature_column\n",
        "2. how to create custom evaluation metrics and add them to Tensorboard\n",
        "3. how to train a model and make predictions with the saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5xpOL_0Ozq"
      },
      "source": [
        "Tensorflow Hub should already be installed. You can check that it is by using \"pip freeze\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-3tesdN0Ozq",
        "outputId": "dc794734-0832-42d9-cf09-46ad708c65e4"
      },
      "source": [
        "%%bash\n",
        "pip freeze | grep tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorboard==2.3.0\n",
            "tensorboard-plugin-wit==1.7.0\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow==2.3.0\n",
            "tensorflow-addons==0.8.3\n",
            "tensorflow-datasets==4.0.1\n",
            "tensorflow-estimator==2.3.0\n",
            "tensorflow-gcs-config==2.3.0\n",
            "tensorflow-hub==0.10.0\n",
            "tensorflow-metadata==0.25.0\n",
            "tensorflow-privacy==0.2.2\n",
            "tensorflow-probability==0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7zo3s20Ozs"
      },
      "source": [
        "Let's make sure we install the necessary version of tensorflow-hub. After doing the pip install below, click **\"Restart the kernel\"** on the notebook so that the Python environment picks up the new packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmt9PRAa0Ozs"
      },
      "source": [
        "!pip3 install tensorflow-hub==0.7.0\n",
        "!pip3 install --upgrade tensorflow==1.15.3\n",
        "#!pip3 install google-cloud-bigquery==1.10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJCcylO10Ozs"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import shutil\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W70tZYP0Ozt"
      },
      "source": [
        "### Build the feature columns for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGEi4Lwv0Ozt"
      },
      "source": [
        "To start, we'll load the list of categories, authors and article ids we created in the previous **Create Datasets** notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT6CcEKP0Ozt",
        "outputId": "b463fa98-1087-443a-8d7b-382bf8a7de6b"
      },
      "source": [
        "categories_list = open(\"categories.txt\").read().splitlines()\n",
        "print(categories_list)\n",
        "authors_list = open(\"authors.txt\").read().splitlines()\n",
        "print(authors_list[:4])\n",
        "content_ids_list = open(\"content_ids.txt\").read().splitlines()\n",
        "print(content_ids_list[:4])\n",
        "mean_months_since_epoch = 523"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['News', 'Stars & Kultur', 'Lifestyle']\n",
            "['Marlene Patsalidis', 'Yvonne Widler', 'Thomas  Trescher', 'Johanna Hager']\n",
            "['299965853', '299972248', '299410466', '299937546']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kakBaa4qHQOK"
      },
      "source": [
        "cols = ['visitor_id'\t, 'content_id'\t,'category'\t, 'title', \t'author',\t'months_since_epoch',\t'next_content_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0RqzAVP5DCPy",
        "outputId": "521f1fe9-9436-4d7d-e28d-65d6d4785b30"
      },
      "source": [
        "import pandas as pd\n",
        "training_set = pd.read_csv('training_set.csv',names=cols)\n",
        "training_set.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visitor_id</th>\n",
              "      <th>content_id</th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>months_since_epoch</th>\n",
              "      <th>next_content_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1042795765758282508</td>\n",
              "      <td>299964154</td>\n",
              "      <td>News</td>\n",
              "      <td>Neue Seidenstraße: Ein chinesischer Keil in Eu...</td>\n",
              "      <td>Hermann Sileitsch-Parzer</td>\n",
              "      <td>574.0</td>\n",
              "      <td>299848776.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1056627016396469139</td>\n",
              "      <td>299821418</td>\n",
              "      <td>Lifestyle</td>\n",
              "      <td>Missbrauchsvorwürfe gegen US-Wellnesskette</td>\n",
              "      <td>Elisabeth Mittendorfer</td>\n",
              "      <td>574.0</td>\n",
              "      <td>299852437.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            visitor_id  content_id  ... months_since_epoch next_content_id\n",
              "0  1042795765758282508   299964154  ...              574.0     299848776.0\n",
              "1  1056627016396469139   299821418  ...              574.0     299852437.0\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1KfK_XQ5irL",
        "outputId": "1387e89f-fe11-4522-80ce-4d80be5b42f9"
      },
      "source": [
        "training_set.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "visitor_id                0\n",
              "content_id                0\n",
              "category               1441\n",
              "title                     1\n",
              "author                38378\n",
              "months_since_epoch        1\n",
              "next_content_id           1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "MbFthlGrGTlv",
        "outputId": "40158b46-2af5-4722-9fc1-b819809fc443"
      },
      "source": [
        "test_set = pd.read_csv('test_set.csv' , names= cols)\n",
        "test_set.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visitor_id</th>\n",
              "      <th>content_id</th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>months_since_epoch</th>\n",
              "      <th>next_content_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1041177675528771456</td>\n",
              "      <td>60544607</td>\n",
              "      <td>Stars &amp; Kultur</td>\n",
              "      <td>Wien Museum: Hier parkt Bruno Kreiskys Rover</td>\n",
              "      <td>Barbara Mader</td>\n",
              "      <td>531</td>\n",
              "      <td>60546015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389130989249956043</td>\n",
              "      <td>299830996</td>\n",
              "      <td>News</td>\n",
              "      <td>Wie die Schule in der Neuzeit ankommen könnte</td>\n",
              "      <td>Martina Salomon</td>\n",
              "      <td>574</td>\n",
              "      <td>299933565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            visitor_id  content_id  ... months_since_epoch next_content_id\n",
              "0  1041177675528771456    60544607  ...                531        60546015\n",
              "1  1389130989249956043   299830996  ...                574       299933565\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-pG9E-70Ozt"
      },
      "source": [
        "In the cell below we'll define the feature columns to use in our model. If necessary, remind yourself the [various feature columns](https://www.tensorflow.org/api_docs/python/tf/feature_column) to use.  \n",
        "For the embedded_title_column feature column, use a Tensorflow Hub Module to create an embedding of the article title. Since the articles and titles are in German, you'll want to use a German language embedding module.  \n",
        "Explore the text embedding Tensorflow Hub modules [available here](https://alpha.tfhub.dev/). Filter by setting the language to 'German'. The 50 dimensional embedding should be sufficient for our purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eArQmv_20Ozt",
        "outputId": "83ebc0f7-edb4-46a8-d7cd-1b923849fd20"
      },
      "source": [
        "embedded_title_column = hub.text_embedding_column(\n",
        "    key=\"title\", \n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-de-dim50/1\",\n",
        "    trainable=False)\n",
        "\n",
        "content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    key=\"content_id\",\n",
        "    hash_bucket_size= len(content_ids_list) + 1)\n",
        "\n",
        "\n",
        "embedded_content_column = tf.feature_column.embedding_column(\n",
        "    categorical_column=content_id_column,\n",
        "    dimension=20)\n",
        "\n",
        "author_column = tf.feature_column.categorical_column_with_hash_bucket(key=\"author\",\n",
        "    hash_bucket_size=len(authors_list) + 1)\n",
        "\n",
        "\n",
        "\n",
        "embedded_author_column = tf.feature_column.embedding_column(\n",
        "    categorical_column=author_column,\n",
        "    dimension=10)\n",
        "\n",
        "category_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    key=\"category\",\n",
        "    vocabulary_list=categories_list,\n",
        "    num_oov_buckets=1)\n",
        "\n",
        "\n",
        "\n",
        "category_column = tf.feature_column.indicator_column(category_column_categorical)\n",
        "\n",
        "months_since_epoch_boundaries = list(range(400,700,20))\n",
        "print(\"buckets : {}\".format(months_since_epoch_boundaries))\n",
        "\n",
        "months_since_epoch_column = tf.feature_column.numeric_column(\n",
        "    key=\"months_since_epoch\")\n",
        "\n",
        "months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
        "    source_column = months_since_epoch_column,\n",
        "    boundaries = months_since_epoch_boundaries)\n",
        "\n",
        "crossed_months_since_category_column = tf.feature_column.indicator_column(tf.feature_column.crossed_column(\n",
        "  keys = [category_column_categorical, months_since_epoch_bucketized], \n",
        "  hash_bucket_size = len(months_since_epoch_boundaries) * (len(categories_list) + 1)))\n",
        "\n",
        "feature_columns = [embedded_content_column,\n",
        "                   embedded_author_column,\n",
        "                   category_column,\n",
        "                   embedded_title_column,\n",
        "                   crossed_months_since_category_column] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buckets : [400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sew3iF9W0Ozt"
      },
      "source": [
        "### Create the input function.\n",
        "\n",
        "Next we'll create the input function for our model. This input function reads the data from the csv files we created in the previous labs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Z6O2ds0Ozt"
      },
      "source": [
        "record_defaults = [[\"Unknown\"], [\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[mean_months_since_epoch],[\"Unknown\"]]\n",
        "column_keys = [\"visitor_id\", \"content_id\", \"category\", \"title\", \"author\", \"months_since_epoch\", \"next_content_id\"]\n",
        "label_key = \"next_content_id\"\n",
        "def read_dataset(filename, mode, batch_size = 1024):\n",
        "  def _input_fn():\n",
        "      def decode_csv(value_column):\n",
        "          columns = tf.io.decode_csv(value_column,record_defaults=record_defaults)\n",
        "          features = dict(zip(column_keys, columns))          \n",
        "          label = features.pop(label_key)         \n",
        "          return features, label\n",
        "\n",
        "      # Create list of files that match pattern\n",
        "      file_list = tf.io.gfile.glob(filename)\n",
        "\n",
        "      # Create dataset from file list\n",
        "      dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "          num_epochs = None # indefinitely\n",
        "          dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "      else:\n",
        "          num_epochs = 1 # end-of-input after this\n",
        "\n",
        "      dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "      return dataset.make_one_shot_iterator().get_next()\n",
        "  return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CmJKKC70Ozt"
      },
      "source": [
        "### Create the model and train/evaluate\n",
        "\n",
        "\n",
        "Next, we'll build our model which recommends an article for a visitor to the Kurier.at website. Look through the code below. We use the input_layer feature column to create the dense input layer to our network. This is just a sigle layer network where we can adjust the number of hidden units as a parameter.\n",
        "\n",
        "Currently, we compute the accuracy between our predicted 'next article' and the actual 'next article' read next by the visitor. We'll also add an additional performance metric of top 10 accuracy to assess our model. To accomplish this, we compute the top 10 accuracy metric, add it to the metrics dictionary below and add it to the tf.summary so that this value is reported to Tensorboard as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmmjCe10Ozt"
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  print(\"No of classes : \", params['n_classes'])\n",
        "  net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
        "  for units in params['hidden_units']:\n",
        "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
        "        print(\"layer : \",net)\n",
        "\n",
        "  \n",
        "   # Compute logits (1 per class).\n",
        "  logits = tf.layers.dense(net, params['n_classes'], activation=None) \n",
        "\n",
        "  predicted_classes = tf.argmax(logits, 1)\n",
        "  from tensorflow.python.lib.io import file_io\n",
        "    \n",
        "  with file_io.FileIO('content_ids.txt', mode='r') as ifp:\n",
        "    content = tf.constant([x.rstrip() for x in ifp])\n",
        "  predicted_class_names = tf.gather(content, predicted_classes)\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        'class_ids': predicted_classes[:, tf.newaxis],\n",
        "        'class_names' : predicted_class_names[:, tf.newaxis],\n",
        "        'probabilities': tf.nn.softmax(logits),\n",
        "        'logits': logits,\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "  table = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"content_ids.txt\")\n",
        "  labels = table.lookup(labels)\n",
        "  # Compute loss.\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "  # Compute evaluation metrics.\n",
        "  accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                 predictions=predicted_classes,\n",
        "                                 name='acc_op')\n",
        "  top_10_accuracy = tf.metrics.mean(tf.nn.in_top_k(predictions=logits, \n",
        "                                                   targets=labels, \n",
        "                                                   k=10))\n",
        "  \n",
        "  metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'top_10_accuracy' : top_10_accuracy}\n",
        "  \n",
        "  tf.summary.scalar('accuracy', accuracy[1])\n",
        "  tf.summary.scalar('top_10_accuracy', top_10_accuracy[1])\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "      return tf.estimator.EstimatorSpec(\n",
        "          mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "  # Create training op.\n",
        "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "  optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8p850er0Ozt"
      },
      "source": [
        "### Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9YuKheu0Ozt",
        "outputId": "d960302b-0042-4a60-d234-e47a7ea01d37"
      },
      "source": [
        "outdir = 'content_based_model_trained'\n",
        "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
        "#tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir = outdir,\n",
        "    params={\n",
        "     'feature_columns': feature_columns,\n",
        "      'hidden_units': [200, 100, 80, 30],\n",
        "      'n_classes': len(content_ids_list)\n",
        "    })\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn = read_dataset(\"training_set.csv\", tf.estimator.ModeKeys.TRAIN),\n",
        "    max_steps = 2000)\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn = read_dataset(\"test_set.csv\", tf.estimator.ModeKeys.EVAL),\n",
        "    steps = None,\n",
        "    start_delay_secs = 30,\n",
        "    throttle_secs = 60)\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'content_based_model_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f22c5daabe0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'content_based_model_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f22c5daabe0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No of classes :  15634\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layer :  Tensor(\"dense/Relu:0\", shape=(?, 200), dtype=float32)\n",
            "layer :  Tensor(\"dense_1/Relu:0\", shape=(?, 100), dtype=float32)\n",
            "layer :  Tensor(\"dense_2/Relu:0\", shape=(?, 80), dtype=float32)\n",
            "layer :  Tensor(\"dense_3/Relu:0\", shape=(?, 30), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into content_based_model_trained/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into content_based_model_trained/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.657534, step = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.657534, step = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79563\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.627903, step = 101 (26.352 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.627903, step = 101 (26.352 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.7774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.7774\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.5912075, step = 201 (26.476 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.5912075, step = 201 (26.476 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.75839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.75839\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.572741, step = 301 (26.603 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.572741, step = 301 (26.603 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.75014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.75014\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.4767475, step = 401 (26.663 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.4767475, step = 401 (26.663 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.8072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.8072\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.119762, step = 501 (26.267 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.119762, step = 501 (26.267 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79103\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.284913, step = 601 (26.381 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.284913, step = 601 (26.381 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.78294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.78294\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 6.030287, step = 701 (26.439 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 6.030287, step = 701 (26.439 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.81217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.81217\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.8275557, step = 801 (26.224 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.8275557, step = 801 (26.224 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.79006\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.4827695, step = 901 (26.388 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.4827695, step = 901 (26.388 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.76141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.76141\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.041579, step = 1001 (26.586 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.041579, step = 1001 (26.586 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.76801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.76801\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.816439, step = 1101 (26.539 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.816439, step = 1101 (26.539 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.77174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.77174\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.854385, step = 1201 (26.513 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.854385, step = 1201 (26.513 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82562\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.939856, step = 1301 (26.140 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.939856, step = 1301 (26.140 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82434\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.720622, step = 1401 (26.145 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.720622, step = 1401 (26.145 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.82544\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.9620523, step = 1501 (26.143 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.9620523, step = 1501 (26.143 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.80688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.80688\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.863186, step = 1601 (26.269 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.863186, step = 1601 (26.269 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.83157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.83157\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.774455, step = 1701 (26.096 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.774455, step = 1701 (26.096 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.83889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.83889\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.6015186, step = 1801 (26.053 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.6015186, step = 1801 (26.053 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.78314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 3.78314\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.974956, step = 1901 (26.434 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.974956, step = 1901 (26.434 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 2000 into content_based_model_trained/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 2000 into content_based_model_trained/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No of classes :  15634\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layer :  Tensor(\"dense/Relu:0\", shape=(?, 200), dtype=float32)\n",
            "layer :  Tensor(\"dense_1/Relu:0\", shape=(?, 100), dtype=float32)\n",
            "layer :  Tensor(\"dense_2/Relu:0\", shape=(?, 80), dtype=float32)\n",
            "layer :  Tensor(\"dense_3/Relu:0\", shape=(?, 30), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2020-12-04T22:50:28Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2020-12-04T22:50:28Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2020-12-04-22:50:34\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2020-12-04-22:50:34\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.02785265, global_step = 2000, loss = 5.3432593, top_10_accuracy = 0.21770382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.02785265, global_step = 2000, loss = 5.3432593, top_10_accuracy = 0.21770382\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 4.78855.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 4.78855.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': 0.02785265,\n",
              "  'global_step': 2000,\n",
              "  'loss': 5.3432593,\n",
              "  'top_10_accuracy': 0.21770382},\n",
              " [])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IGEZUUr0Ozu"
      },
      "source": [
        "This takes a while to complete but in the end, I get about **30% top 10 accuracy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeSXwS5E0Ozu"
      },
      "source": [
        "### Make predictions with the trained model. \n",
        "\n",
        "With the model now trained, we can make predictions by calling the predict method on the estimator. Let's look at how our model predicts on the first five examples of the training set.  \n",
        "To start, we'll create a new file 'first_5.csv' which contains the first five elements of our training set. We'll also save the target values to a file 'first_5_content_ids' so we can compare our results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4KQcWh0Ozu",
        "outputId": "05eb20bd-b2fd-48fd-8e01-d905e589e1de"
      },
      "source": [
        "%%bash\n",
        "head -30 training_set.csv > first_30.csv\n",
        "head first_30.csv\n",
        "awk -F \"\\\"*,\\\"*\" '{print $2}' first_30.csv > first_30_content_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1042795765758282508,299964154,News,Neue Seidenstraße: Ein chinesischer Keil in Europa,Hermann Sileitsch-Parzer,574,299848776\n",
            "1056627016396469139,299821418,Lifestyle,Missbrauchsvorwürfe gegen US-Wellnesskette,Elisabeth Mittendorfer,574,299852437\n",
            "106377193142039719,299912085,News,Erster ÖBB-Containerzug nach China unterwegs,Stefan Hofer,574,299910994\n",
            "106377193142039719,299813480,Lifestyle,Alice Schwarzer: Periode des Rückschlags für Frauen,Elisabeth Mittendorfer,574,299939900\n",
            "106377193142039719,299918278,News,Skipässe in Wintersport-Hochburgen massiv teurer,Stefan Hofer,574,299853016\n",
            "106377193142039719,299918253,News,Ringen: Iraner musste verlieren um Duell mit Israeli zu vermeiden,Mirad Odobasic,574,299913879\n",
            "106377193142039719,299913879,News,Python erwürgte thailändischen Besitzer,,574,299907204\n",
            "106377193142039719,299902870,News,RAF-Terroristin bittet Schleyer-Familie um Verzeihung,Stefan Hofer,574,299935287\n",
            "106377193142039719,299444828,Lifestyle,RunNa: Läufst du noch oder kotzt du schon?,Natascha Marakovits,574,299295158\n",
            "1072182984311253795,299792812,News,Bundesliga: Kein Videobeweis beim Schlager Rapid-Salzburg,,574,299861625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUBF7qMs0Ozu"
      },
      "source": [
        "Recall, to make predictions on the trained model we pass a list of examples through the input function. Complete the code below to make predicitons on the examples contained in the \"first_5.csv\" file we created above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMWlvJR3B1U3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn_0GMDd0Ozu",
        "outputId": "5c59e3ce-931f-41cb-c855-4658a830eafc"
      },
      "source": [
        "output = list(estimator.predict(input_fn=read_dataset(\"first_30.csv\", tf.estimator.ModeKeys.PREDICT)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No of classes :  15634\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layer :  Tensor(\"dense/Relu:0\", shape=(?, 200), dtype=float32)\n",
            "layer :  Tensor(\"dense_1/Relu:0\", shape=(?, 100), dtype=float32)\n",
            "layer :  Tensor(\"dense_2/Relu:0\", shape=(?, 80), dtype=float32)\n",
            "layer :  Tensor(\"dense_3/Relu:0\", shape=(?, 30), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from content_based_model_trained/model.ckpt-2000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sb4c59r0Ozu",
        "outputId": "3630f27e-c8d0-447e-93a2-3ee3b3c44343"
      },
      "source": [
        "import numpy as np\n",
        "recommended_content_ids = [np.asscalar(d[\"class_names\"]).decode('UTF-8') for d in output]\n",
        "content_ids = open(\"first_30_content_ids\").read().splitlines()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUE1OFFk9Xlf",
        "outputId": "b8cf37c1-417f-47d9-ce54-0e8f25b7112b"
      },
      "source": [
        "print(recommended_content_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775', '299826775']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02LKLt40Ozu"
      },
      "source": [
        "Finally, we map the content id back to the article title. Let's compare our model's recommendation for the first example. This can be done in BigQuery. Look through the query below and make sure it is clear what is being returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiYpGQKr0Ozu"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "recommended_title_sql=\"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
        "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
        "  UNNEST(hits) AS hits\n",
        "WHERE \n",
        "  # only include hits on pages\n",
        "  hits.type = \"PAGE\"\n",
        "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
        "LIMIT 1\"\"\".format(recommended_content_ids[0])\n",
        "\n",
        "current_title_sql=\"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
        "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
        "  UNNEST(hits) AS hits\n",
        "WHERE \n",
        "  # only include hits on pages\n",
        "  hits.type = \"PAGE\"\n",
        "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
        "LIMIT 1\"\"\".format(content_ids[0])\n",
        "recommended_title = bigquery.Client().query(recommended_title_sql).to_dataframe()['title'].tolist()[0].encode('utf-8').strip()\n",
        "current_title = bigquery.Client().query(current_title_sql).to_dataframe()['title'].tolist()[0].encode('utf-8').strip()\n",
        "print(\"Current title: {} \".format(current_title))\n",
        "print(\"Recommended title: {}\".format(recommended_title))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7aqsib40Ozu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}